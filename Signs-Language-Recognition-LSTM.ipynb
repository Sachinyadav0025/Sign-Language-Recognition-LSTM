{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a1af34-328a-4c62-93ed-cb22d717bb6b",
   "metadata": {},
   "source": [
    "# import and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea9b02-cedc-4b96-bbf3-adbbacfe93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"OpenCV version:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d3294-5ff8-4be5-be9f-510e087f57a3",
   "metadata": {},
   "source": [
    "# keypoits using mp holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db09d2-50cf-4de8-8209-0893d963b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic  # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils  # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccaa1fb-8be0-467e-809f-5493c784da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False                 \n",
    "    results = model.process(image)                 \n",
    "    image.flags.writeable = True                  \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d3ec9-e289-45af-8fba-5230340b8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    #  face connections \n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) \n",
    "    #  pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) \n",
    "    #  left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "    #  right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966568e6-f7cf-4ecd-82a0-bc8f39295750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "  \n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(224, 224, 224), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1, circle_radius=1)  \n",
    "                             )\n",
    "    \n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(57, 255, 20), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(0, 128, 0), thickness=2, circle_radius=2)    \n",
    "                             )\n",
    "    \n",
    "    \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=2, circle_radius=4),  \n",
    "                             mp_drawing.DrawingSpec(color=(128, 128, 0), thickness=2, circle_radius=2)   \n",
    "                             )\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=2, circle_radius=4),  \n",
    "                             mp_drawing.DrawingSpec(color=(128, 0, 128), thickness=2, circle_radius=2)  \n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581f469-6d6b-4b49-9758-46a480628202",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        \n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "    \n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b22219-274a-4bdc-b9a9-28b4c010c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.left_hand_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f16d4-7fde-4145-98c7-f7f021cd129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7ee5c-5ca2-45c9-84ae-9fbb3003ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draw_styled_landmarks(frame, results)\n",
    "\n",
    "\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f93b4d-3219-4f63-b058-e6439dcc236d",
   "metadata": {},
   "source": [
    "# extract folders for collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a79a5-d6b6-4408-a729-3a6f16a930e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.pose_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3676f2e-fbc0-4065-82ca-70c526c84820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    # 1. Pose: 33 landmarks * 4 values (x, y, z, visibility) = 132 values\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    \n",
    "    # 2. Face: 468 landmarks * 3 values (x, y, z) = 1404 values\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    \n",
    "    # 3. Left Hand: 21 landmarks * 3 values (x, y, z) = 63 values\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    # 4. Right Hand: 21 landmarks * 3 values (x, y, z) = 63 values\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    # 5. Concatenate everything into one big array (Total: 1662 values)\n",
    "    return np.concatenate ([pose,face,lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e594e0-efeb-43b6-82ad-0103f8b43568",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39360d48-d7cb-4203-ae8b-aeffb158d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce5b1b-a22e-43cd-8812-ab84a7e9d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('0',result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9f0b1-1029-49e9-b18c-a29fbfe518c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('0.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce2857-684c-4c4c-8b55-beb9daaf1788",
   "metadata": {},
   "source": [
    "# setup folders for collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165202e1-5e88-4348-ab94-eb4632515af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "actions = np.load(\"actions.npy\")\n",
    "\n",
    "\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "\n",
    "\n",
    "no_sequences = 30\n",
    "sequence_length = 30\n",
    "\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        os.makedirs(os.path.join(DATA_PATH, action, str(sequence)), exist_ok=True)\n",
    "\n",
    "print(\"Folders verified using frozen actions:\", actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e591b21-8020-46eb-8256-7faa89db8b7c",
   "metadata": {},
   "source": [
    "# collect keypoints values for training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef51849-7a41-4361-a658-1b3ad0e1a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    stop_recording = False \n",
    "    \n",
    "    for action in actions:\n",
    "        if stop_recording: break \n",
    "            \n",
    "        for sequence in range(no_sequences):\n",
    "            if stop_recording: break \n",
    "                \n",
    "            for frame_num in range(sequence_length):\n",
    "                ret, frame = cap.read()\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                \n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Collecting {action} Video {sequence}', (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000) \n",
    "                else: \n",
    "                    cv2.putText(image, f'Collecting {action} Video {sequence}', (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                \n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                \n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    stop_recording = True\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece83124-e8fb-43e7-8f01-0f7f3a405397",
   "metadata": {},
   "source": [
    "# preprocess data and create lables and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00649f12-e0a7-4b4c-9e92-bfe1e6502118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e32e19-a233-460e-8391-f15d98450f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label : num for num , label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef8a2d-0a06-480b-a051-bd0a76edf0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b71e1-078c-4a40-8cd7-7e4e72e0c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                          this code is for loading actions so that we do not have to recapture the frames again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36e149-b364-47af-9b8d-8dec1482c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,labels = [],[]                                 \n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "     window = []\n",
    "     for frame_num in range(sequence_length):\n",
    "        res = np.load(os.path.join(DATA_PATH , action , str(sequence), \"{}.npy\".format(frame_num)))\n",
    "        window.append(res)\n",
    "     sequences.append(window)\n",
    "     labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3808079-3dbd-4969-9774-5584a6ad528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714f89c-d07e-495c-bda1-429db42620c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf5ae8-d14d-4cbd-85aa-a380226f313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d1410-65a2-4dfa-a4ae-f61c8163b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a958e-90b5-4e47-9a09-2e0cdd5f8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels, num_classes=len(actions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309601ee-a45e-4914-8454-8a6a81f36149",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da613c-e5de-4f3f-aa06-fed8c9405e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=y.argmax(axis=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d34b731-64bc-42d9-b4b1-d97a4402ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17132883-23a4-4b34-a2e7-5606d4117f0f",
   "metadata": {},
   "source": [
    "# Build and  Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7b003-0459-405c-981f-f42979f96f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba5a9-6f50-40f8-bace-1c12d6a178c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir = log_dir)  # use to monitor our neural network train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f53f264-ec44-4377-a918-04a94a46a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences = True,activation = 'relu',input_shape = (30,1662)))\n",
    "model.add(LSTM(128, return_sequences = True,activation = 'relu'))\n",
    "model.add(LSTM(64, return_sequences = False,activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(actions.shape[0],activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef5f9c-d684-479d-987c-6b10576cf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [.7,0.2,0.1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e608b8f-f76b-401f-912a-cc87e0d762fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f9461-6180-4e2e-aa0f-28d90e606985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e8bb6-0cb2-4d1b-b9c9-eb2876b41cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,epochs=2000, callbacks = [tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fce3e6-0522-4f6f-bc0f-2ac451585bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model so you don't have to train it again\n",
    "model.save('action.h5')\n",
    "print(\" Model saved as action.h5!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6276b-73e0-4587-a665-7bf1dbf5d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044bb84-d171-4416-a788-5131fe64f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90dc902-0c4d-4d35-ab21-1ff719bbe803",
   "metadata": {},
   "source": [
    "# make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3961c8-31db-43e7-90c6-2f09ad6ae3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0341f-b122-4a45-a8bd-300d0e823ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff9621-5f4c-44e7-8696-ffe97cbe4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac22676-65e4-453d-8ed6-fc15b9cff2d4",
   "metadata": {},
   "source": [
    "# Evaluation using Confussion matrix and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b4e6b-9863-42ee-80cf-f39ab5a7f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb9b40-c634-4def-bb19-20639517c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da980b-32b4-4790-815a-927e53de66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test,axis = 1).tolist()\n",
    "yhat = np.argmax(yhat,axis = 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5bacd5-b40d-49e9-9fbc-a44613290781",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dcb509-c49f-4470-bcd7-1bb2bfbe48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf3a50-c897-4076-aa14-f0fb1d731016",
   "metadata": {},
   "source": [
    "# test in real time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e0863-2592-4ef5-8680-7f4f79b382af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need 6 colors for 6 signs\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245), (255,0,0), (0,255,0), (0,0,255)]\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "    \n",
    "        cv2.rectangle(output_frame, (0, 60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a89d3-1907-4f0d-a0b6-c41255c22cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(res.shape) > 1:\n",
    "    res = res[0]\n",
    "\n",
    "\n",
    "if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "   \n",
    "    processed_image = prob_viz(res, actions, image, colors)\n",
    "    plt.imshow(processed_image)\n",
    "else:\n",
    "    print(\"Error: No hands detected in the current frame to visualize!\")\n",
    "    plt.imshow(image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebc4f9-5096-4994-b9f9-0ab5482007d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.6  \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        \n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:] \n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "            if len(predictions) >= 5:\n",
    "                if np.unique(predictions[-5:])[0] == np.argmax(res): \n",
    "                    if res[np.argmax(res)] > threshold: \n",
    "                        if len(sentence) > 0: \n",
    "                            if actions[np.argmax(res)] != sentence[-1]:\n",
    "                                sentence.append(actions[np.argmax(res)])\n",
    "                        else:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "            \n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('Fast Detection - 6 Signs', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4e3b1-9ede-40c3-92c0-3c97ed644eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37890d-5b1e-4572-a6fb-85884fa7f10c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sign Language (Final)",
   "language": "python",
   "name": "sign_lang_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
